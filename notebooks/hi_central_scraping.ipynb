{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f67647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.225315Z",
     "start_time": "2024-05-21T12:25:45.174612Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Get latitude and Longitude of properties\n",
    "from geopy.geocoders import GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "# Logging\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd41939",
   "metadata": {},
   "source": [
    "# Read in Google Maps API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca50ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.230960Z",
     "start_time": "2024-05-21T12:25:46.226327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in our Google Maps API key\n",
    "with open('../locker/.env', 'r') as env_file:\n",
    "    lines = env_file.readlines()\n",
    "    raw_api = lines[0]\n",
    "\n",
    "GMAP_API_KEY = raw_api.split('=')[1].replace(\"'\", \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0bdc9",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646846fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.247459Z",
     "start_time": "2024-05-21T12:25:46.232229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "OUT_DIR = '../results'\n",
    "\n",
    "# All results URL\n",
    "BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "\n",
    "# Direct property URL\n",
    "DIRECT_URL = f'{BASE_URL}/HBR/ForRent/?/'\n",
    "\n",
    "# HI Central has a very weird URL style, from what I've been able to decipher \n",
    "# for every filter there is a section in the URL, no matter if that filter is\n",
    "# applied, in which case there will be nothing beteween the \"start\" and \"end\"\n",
    "# slashes, e.g. '//', or if a filter is applied then a number will appear\n",
    "# between the slashes, e.g., '/295/'. So this leads to some really wonky URLs\n",
    "# for scraping as can be seen below.\n",
    "\n",
    "all_res_url = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood///295//128////////1////////////////////////////'\n",
    "\n",
    "\n",
    "START_URL = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood//'\n",
    "\n",
    "MID_URL = f'/295//128////////'\n",
    "END_URL = '////////////////////////'\n",
    "\n",
    "SHORT_END_URL = '////////////////////////'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bf5b3",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70873c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.261911Z",
     "start_time": "2024-05-21T12:25:46.249465Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def get_dt():\n",
    "    now = datetime.now().astimezone()\n",
    "    dt_string = now.strftime('%Y-%m-%d_%H-%M-%S_%Z')\n",
    "    dt_string = dt_string.replace('Eastern Daylight Time', 'EDT')\n",
    "    dt_string = dt_string.replace('Eastern Standard Time', 'EST')\n",
    "    return dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5ecfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.273588Z",
     "start_time": "2024-05-21T12:25:46.262918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_page(input_url):\n",
    "\n",
    "    page = req.get(input_url)\n",
    "    soup = bs4(page.content, \"html.parser\")\n",
    "\n",
    "    curr_ids = [anchor.text for anchor in soup.find_all('a', string=re.compile(r'^\\d+$'))]\n",
    "\n",
    "    # Capture when there are additional pages of results\n",
    "    page_num_ls = list(set([x for x in curr_ids if x.isdigit() and 1 <= int(x) <= 10]))\n",
    "    n_pages = len(page_num_ls)\n",
    "\n",
    "    # Grab the IDs without the page numbers\n",
    "    curr_ids = list(set([x for x in curr_ids if x.isdigit() and 1 <= int(x) > 10]))\n",
    "    curr_id_ls = list(zip([region] * len(curr_ids), curr_ids))\n",
    "    curr_id_ls = [list(t) for t in curr_id_ls]\n",
    "    \n",
    "    \n",
    "    return [curr_id_ls, page_num_ls, n_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc58890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.281983Z",
     "start_time": "2024-05-21T12:25:46.274631Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gen_url(page_num = '', option_id = '', min_dollar = '', max_dollar = ''):\n",
    "    \n",
    "    BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "    START_URL = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood//'\n",
    "    MID_URL = f'/295//128////////'\n",
    "    SHORT_END_URL = '////////////////////////'\n",
    "\n",
    "    ret_url = f'{START_URL}{page_num}{MID_URL}1/{option_id}///{min_dollar}/{max_dollar}/{SHORT_END_URL}'\n",
    "    \n",
    "    return ret_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fa87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.292193Z",
     "start_time": "2024-05-21T12:25:46.283008Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Just see if it starts with numbers like an address (123 Brighton street | Not Honolulu, HI 94666)\n",
    "def is_full_add(add_text):\n",
    "    ADD_REGEX = r'^\\d'\n",
    "    return re.search(ADD_REGEX, add_text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c296fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.302125Z",
     "start_time": "2024-05-21T12:25:46.293232Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "def break_address(input_str):\n",
    "    UNIT_REGEX = r'#\\S+'\n",
    "\n",
    "    # Parts of address\n",
    "    loc_name = '-'\n",
    "    street_add = '-'\n",
    "    rest = '-'\n",
    "    unit_num = '-'\n",
    "    city = '-'\n",
    "    state = '-'\n",
    "    zip_code = '-'\n",
    "\n",
    "    if input_str == 'Address unavailable':\n",
    "        return  loc_name, street_add, rest, unit_num, city, state, zip_code \n",
    "\n",
    "    add_parts = input_str.split('\\n')\n",
    "\n",
    "    for curr_part in add_parts:\n",
    "        # Check if the part contains any digits\n",
    "        if any(char.isdigit() for char in curr_part):\n",
    "            # If it contains digits, it's the address or rest of address with zip code\n",
    "\n",
    "            if is_full_add(curr_part):\n",
    "                street_add = curr_part\n",
    "            else:\n",
    "                rest = curr_part\n",
    "        else:\n",
    "            # If it doesn't contain digits, it's likely the location name\n",
    "            loc_name = curr_part\n",
    "\n",
    "    if street_add != '-':\n",
    "        if '#' in street_add:\n",
    "            \n",
    "            # We have an apartment number, so parse and set it\n",
    "            hash_idx = street_add.find('#')\n",
    "            unit_num = f'#{street_add[hash_idx:]}'\n",
    "\n",
    "    if rest != '-':\n",
    "        city, state_zip = rest.strip().split(',')\n",
    "\n",
    "        state, zip_code = state_zip.strip().split(' ')\n",
    "\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        print(input_str)\n",
    "        print(f\"\\tloc_name   =  {loc_name}\")\n",
    "        print(f\"\\tstreet_add =  {street_add}\")\n",
    "        print(f\"\\tunit_num   =  {unit_num}\")\n",
    "        print(f\"\\tcity       =  {city}\")\n",
    "        print(f\"\\tstate      =  {state}\")\n",
    "        print(f\"\\tzip_code   =  {zip_code}\")\n",
    "        print(\"============================================================\")\n",
    "\n",
    "\n",
    "\n",
    "    return  loc_name, street_add, rest, unit_num, city, state, zip_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262abca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.313460Z",
     "start_time": "2024-05-21T12:25:46.303130Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_id_page_w_api(input_id):\n",
    "\n",
    "    BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "    curr_url = f'{BASE_URL}/HBR/ForRent/?/{input_id}'\n",
    "    \n",
    "    # Define the number of maximum retries\n",
    "    max_retries = 7\n",
    "    \n",
    "    # Create a Session object\n",
    "    session = req.Session()\n",
    "\n",
    "    # Setup retry strat\n",
    "    retry_strategy = Retry(total = 5, backoff_factor = 10)\n",
    "\n",
    "    # Create a HTTPAdapter with the specified max retries\n",
    "    adapter = HTTPAdapter(max_retries = retry_strategy)\n",
    "    \n",
    "    # Mount the adapter to the session\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    \n",
    "    page = session.get(curr_url)\n",
    "    soup = bs4(page.content, \"html.parser\")\n",
    "    \n",
    "    # Check if listing was not found\n",
    "    header = soup.find('h2', class_ = 'hdr-sub')\n",
    "    \n",
    "    \n",
    "    content_div = soup.find('div', id = 'content')\n",
    "    soup.contents = content_div.contents\n",
    "    \n",
    "    add_text = soup.h2.get_text(separator=\"\\n\")\n",
    "    \n",
    "    # Break address into parts \n",
    "    loc_name, street_add, rest, unit_num, city, state, zip_code  = break_address(add_text)\n",
    "\n",
    "    GOOD_ADD = False\n",
    "    if ((street_add != '-') and (city != '-') and (state != '-') and (zip_code != '-')):\n",
    "        clean_full_add = f'{street_add}, {city}, {state} {zip_code}'\n",
    "        GOOD_ADD = True\n",
    "    else:\n",
    "        clean_full_add = '-'\n",
    "    \n",
    "    \n",
    "    if GOOD_ADD:\n",
    "        lat, long = get_coords(clean_full_add, GMAP_API_KEY)\n",
    "    else:\n",
    "        lat = '-'\n",
    "        long = '-'\n",
    "    \n",
    "    \n",
    "    sub = soup.find('div', class_ = 'sub-heading')\n",
    "\n",
    "    active_status = sub.find('div', class_ = 'active-box')\n",
    "    active_status_str = active_status.text\n",
    "\n",
    "    price_boxes = sub.find_all('div', class_ = 'price-box')\n",
    "    for curr_box in price_boxes:\n",
    "\n",
    "\n",
    "        curr_label = curr_box.find('span').text\n",
    "\n",
    "\n",
    "        if curr_label == 'Price':\n",
    "            curr_price = curr_box.find('div').text\n",
    "\n",
    "        elif curr_label == 'Rental Type':\n",
    "            curr_type = curr_box.find('div').text\n",
    "\n",
    "        elif curr_label == 'Available Date':\n",
    "            curr_date = curr_box.find('div').text\n",
    "\n",
    "\n",
    "\n",
    "        bedrooms = soup.find('dt', string = 'Bedrooms: ').find_next_sibling('dd').text\n",
    "        full_baths = soup.find('dt', string = 'Full Baths: ').find_next_sibling('dd').text\n",
    "        half_baths = soup.find('dt', string = 'Half Baths: ').find_next_sibling('dd').text\n",
    "        parking = soup.find('dt', string = 'Parking Stalls: ').find_next_sibling('dd').text\n",
    "\n",
    "        land_area  = soup.find('dt', string = 'Land Area (sf): ').find_next_sibling('dd').text\n",
    "        live_area  = soup.find('dt', string = 'Living (sf): ').find_next_sibling('dd').text\n",
    "        lanai_area = soup.find('dt', string = 'Lanai (sf): ').find_next_sibling('dd').text\n",
    "        other_area = soup.find('dt', string = 'Other (sf): ').find_next_sibling('dd').text\n",
    "\n",
    "        island = soup.find('dt', string = 'Island:').find_next_sibling('dd').text.title()\n",
    "        region = soup.find('dt', string = 'Region:').find_next_sibling('dd').text.title()\n",
    "        hood = soup.find('dt', string = 'Neighborhood:').find_next_sibling('dd').text.title()\n",
    "\n",
    "        pets_allowed =  soup.find('dt', string = 'Pets Allowed?').find_next_sibling('dd').text\n",
    "        res_man =  soup.find('dt', string = 'Resident Manager?').find_next_sibling('dd').text\n",
    "\n",
    "\n",
    "        deposit =  soup.find('dt', string = 'Deposit Amount:').find_next_sibling('dd').text\n",
    "        term =  soup.find('dt', string = 'Terms Accepted:').find_next_sibling('dd').text\n",
    "\n",
    "        description = soup.find('h3', string = 'REMARKS:').find_next_sibling('p').text\n",
    "\n",
    "        unit_features =  soup.find('dt', string = 'Unit Features:').find_next_sibling('dd').text\n",
    "        parking_features =  soup.find('dt', string = 'Parking Features:').find_next_sibling('dd').text\n",
    "        frontage = soup.find('dt', string = 'Frontage:').find_next_sibling('dd').text\n",
    "        view = soup.find('dt', string = 'View:').find_next_sibling('dd').text\n",
    "        furnished = soup.find('dt', string = 'Furnished: ').find_next_sibling('dd').text\n",
    "        amenities = soup.find('dt', string = 'Amenities:').find_next_sibling('dd').text\n",
    "        pool = soup.find('dt', string = 'Pool:').find_next_sibling('dd').text\n",
    "        inclusions = soup.find('dt', string = 'Inclusions:').find_next_sibling('dd').text\n",
    "        build_style = soup.find('dt', string = 'Building Style:').find_next_sibling('dd').text\n",
    "        disclosures = soup.find('dt', string = 'Disclosures:').find_next_sibling('dd').text\n",
    "\n",
    "        img_urls = []\n",
    "        for img in soup.find_all('img', {'u': 'image'}):\n",
    "            img_urls.append(f\"https:{img['src']}\")\n",
    "\n",
    "    return [input_id, clean_full_add, GOOD_ADD, lat, long, loc_name, \n",
    "                 street_add, unit_num, city, state, zip_code, active_status_str, curr_price, curr_type, curr_date,\n",
    "                 bedrooms, full_baths, half_baths, parking, \n",
    "                 land_area, live_area, lanai_area, other_area,\n",
    "                 island, region, hood, pets_allowed, res_man, deposit, term, description,\n",
    "                 unit_features, parking_features, frontage, view, furnished, amenities, pool, inclusions, \n",
    "                 build_style, disclosures, img_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109aa3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.322716Z",
     "start_time": "2024-05-21T12:25:46.315476Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_coords(input_address, input_api_key):\n",
    "    try:\n",
    "        geolocator = GoogleV3(api_key = input_api_key)\n",
    "        location = geolocator.geocode(input_address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        else:\n",
    "            return None\n",
    "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348cd90",
   "metadata": {},
   "source": [
    "# Collect all property IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cd76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:26:19.369797Z",
     "start_time": "2024-05-21T12:25:46.323757Z"
    },
    "code_folding": [
     0,
     6,
     21,
     29
    ]
   },
   "outputs": [],
   "source": [
    "# Get datetime\n",
    "curr_dt = get_dt()\n",
    "\n",
    "\n",
    "# It will max give me 10 pages of Apartments, so max of 200, so I'm going to need to \n",
    "# do some limitations \n",
    "region_dict = {\n",
    "                    \"1\": \"All Regions\",\n",
    "                    \"2\": \"Diamond Head\",\n",
    "                    \"3\": \"Ewa Plain\",\n",
    "                    \"4\": \"Hawaii Kai\",\n",
    "                    \"5\": \"Kailua\",\n",
    "                    \"6\": \"Kaneohe\",\n",
    "                    \"7\": \"Leeward Coast\",\n",
    "                    \"8\": \"Makakilo\",\n",
    "                    \"9\": \"Metro Oahu\",\n",
    "                    \"10\": \"North Shore\",\n",
    "                    \"11\": \"Pearl City/Aiea\",\n",
    "                    \"12\": \"Waipahu\"\n",
    "}\n",
    "\n",
    "DOLLAR_SEARCH_LS = [['', 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], \n",
    "                    [6, 7], [7, 8], [8, 9], [9, 10], [10, '']]\n",
    "\n",
    "fin_id_ls = []\n",
    "reg_sum_ls = []\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "for option_id, region in tqdm.tqdm(region_dict.items()):\n",
    "    \n",
    "    region_id_ls = []\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(f'{option_id} -> {region}')\n",
    "    \n",
    "    # Skip the all regions option since we can't get all results\n",
    "    if region == 'All Regions':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    curr_url = gen_url(option_id = option_id)\n",
    "   \n",
    "    curr_id_ls, page_num_ls, n_pages = get_page(curr_url)\n",
    "\n",
    "    n_ids = len(curr_id_ls)\n",
    "    region_id_ls.append(curr_id_ls)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(f'\\tURL:\\t{curr_url}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "    \n",
    "    \n",
    "    # If we have more pages to look at grab those too\n",
    "    if len(page_num_ls) > 0:\n",
    "        \n",
    "\n",
    "        for curr_page in page_num_ls:\n",
    "            \n",
    "            \n",
    "            curr_url = gen_url(page_num = curr_page, option_id = option_id)\n",
    "\n",
    "            curr_id_ls, page_num_ls, n_pages = get_page(curr_url)\n",
    "\n",
    "            n_ids = n_ids + len(curr_id_ls)\n",
    "            region_id_ls.append(curr_id_ls)\n",
    "    \n",
    "            if DEBUG:\n",
    "                print(f'More than 1 page:\\n\\tURL:\\t{curr_url}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "        \n",
    "    # Metro oahu also has more than 200 properties, so we need to figure out a way\n",
    "    # to break that up \n",
    "    # We can walk different searches\n",
    "    if n_ids == 200:\n",
    "        #PAGE_URL = f'{START_URL}{PAGE_NUM}{MID_URL}1/{option_id}///{MIN_DOLLAR}/MAX_DOLLAR/{SHORT_END_URL}'\n",
    "\n",
    "        for MIN_DOLLAR, MAX_DOLLAR in DOLLAR_SEARCH_LS:\n",
    "            # Reset page number\n",
    "            curr_url = gen_url(option_id = option_id, min_dollar = MIN_DOLLAR, max_dollar = MAX_DOLLAR)\n",
    "\n",
    "            curr_id_ls, page_num_ls, n_pages = get_page(curr_url)\n",
    "\n",
    "            n_ids = len(curr_id_ls)\n",
    "            region_id_ls.append(curr_id_ls)\n",
    "            if DEBUG:\n",
    "                print(f'More than 200 IDs:\\n\\tURL:\\t{curr_url}\\n\\tMin Dollar:\\t{MIN_DOLLAR}\\n\\tMax Dollar:\\t{MAX_DOLLAR}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "\n",
    "            # If we have more pages to look at grab those too\n",
    "            if len(page_num_ls) > 0:\n",
    "\n",
    "                for curr_page in page_num_ls:\n",
    "                    curr_url = gen_url(page_num = curr_page, option_id = option_id,\n",
    "                                       min_dollar = MIN_DOLLAR, max_dollar = MAX_DOLLAR)\n",
    "\n",
    "                    curr_id_ls, page_num_ls, n_pages = get_page(curr_url)\n",
    "\n",
    "                    n_ids = n_ids + len(curr_id_ls)\n",
    "                    region_id_ls.append(curr_id_ls)\n",
    "    \n",
    "                if DEBUG:\n",
    "                    print(f'More than 200 IDs and more than 1 page:\\n\\tURL:\\t{curr_url}\\n\\tMin Dollar:\\t{MIN_DOLLAR}\\n\\tMax Dollar:\\t{MAX_DOLLAR}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "    \n",
    "    reg_sum_ls.append([region, n_pages, n_ids])\n",
    "    fin_id_ls.append(region_id_ls)\n",
    "    if DEBUG:\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "# ChatGPT's interesting way of un-nesting this mess\n",
    "unnested_list = [inner for outer in fin_id_ls for middle in outer for inner in middle]\n",
    "fin_id_ls = unnested_list\n",
    "\n",
    "fin = pd.DataFrame(fin_id_ls, columns = ['region', 'id'])\n",
    "\n",
    "# Drop any dupes that were collected\n",
    "fin = fin.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29a2d7",
   "metadata": {},
   "source": [
    "# Get info about each property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e534719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:35:42.795761Z",
     "start_time": "2024-05-21T12:26:19.370828Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Collect the actual home data\n",
    "fin_id_ls = fin['id'].unique().tolist()\n",
    "\n",
    "home_data_ls = []\n",
    "failed_ls = []\n",
    "for curr_id in tqdm.tqdm(fin_id_ls):\n",
    "\n",
    "    curr_url = f'{BASE_URL}/HBR/ForRent/?/{curr_id}'\n",
    "    fail = True\n",
    "    try:\n",
    "        curr_page_dat = get_id_page_w_api(curr_id)\n",
    "        fail = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed on {curr_id}:\\t{curr_url}\\n\\t\\t{e}')   \n",
    "\n",
    "        fail = True\n",
    "\n",
    "    if fail:\n",
    "        failed_ls.append(curr_id)\n",
    "    else:        \n",
    "        home_data_ls.append(curr_page_dat)\n",
    "        \n",
    "\n",
    "home_data = pd.DataFrame(home_data_ls, \n",
    "                        columns = ['input_id', 'clean_full_add', 'is_good_addy', 'lat', 'long',\n",
    "                                   'loc_name', 'street_add', 'unit', 'city', 'state', 'zip_code', \n",
    "                                   'active_status_str', 'curr_price', 'curr_type', 'curr_date',\n",
    "                                   'bedrooms', 'full_baths', 'half_baths', 'parking', \n",
    "                                   'land_area', 'live_area', 'lanai_area', 'other_area',\n",
    "                                   'island', 'region', 'hood', 'pets_allowed', 'res_man', 'deposit', 'term', 'description',\n",
    "                                   'unit_features', 'parking_features', 'frontage', 'view', 'furnished', \n",
    "                                   'amenities', 'pool', 'inclusions', \n",
    "                                   'build_style', 'disclosures', 'img_urls'])\n",
    "\n",
    "\n",
    "# Save the output of this run\n",
    "home_data.to_csv(f'{OUT_DIR}/hi_central_data_{curr_dt}.tsv', sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
