{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f67647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.225315Z",
     "start_time": "2024-05-21T12:25:45.174612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dealing with web stuff\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "\n",
    "# Get latitude and Longitude of properties\n",
    "from geopy.geocoders import GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "# Misc libraries\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd41939",
   "metadata": {},
   "source": [
    "# Read in Google Maps API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceca50ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.230960Z",
     "start_time": "2024-05-21T12:25:46.226327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path of script\n",
    "env_path = '../locker/.env'\n",
    "\n",
    "# Read in our Google Maps API key\n",
    "with open(env_path, 'r') as env_file:\n",
    "    lines = env_file.readlines()\n",
    "    raw_api = lines[0]\n",
    "\n",
    "GMAP_API_KEY = raw_api.split('=')[1].replace(\"'\", \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0bdc9",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646846fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.247459Z",
     "start_time": "2024-05-21T12:25:46.232229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "OUT_DIR = '../scraping_results'\n",
    "\n",
    "# All results URL\n",
    "BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "\n",
    "# Direct property URL\n",
    "DIRECT_URL = f'{BASE_URL}/HBR/ForRent/?/'\n",
    "\n",
    "# HI Central has a very weird URL style, from what I've been able to decipher \n",
    "# for every filter there is a section in the URL, no matter if that filter is\n",
    "# applied, in which case there will be nothing beteween the \"start\" and \"end\"\n",
    "# slashes, e.g. '//', or if a filter is applied then a number will appear\n",
    "# between the slashes, e.g., '/295/'. So this leads to some really wonky URLs\n",
    "# for scraping as can be seen below.\n",
    "\n",
    "all_res_url = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood///295//128////////1////////////////////////////'\n",
    "\n",
    "START_URL = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood//'\n",
    "\n",
    "MID_URL = f'/295//128////////'\n",
    "END_URL = '////////////////////////'\n",
    "\n",
    "SHORT_END_URL = '////////////////////////'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bf5b3",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70873c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.261911Z",
     "start_time": "2024-05-21T12:25:46.249465Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def get_dt():\n",
    "    now = datetime.now().astimezone()\n",
    "    dt_string = now.strftime('%Y-%m-%d_%H-%M-%S_%Z')\n",
    "    dt_string = dt_string.replace('Eastern Daylight Time', 'EDT')\n",
    "    dt_string = dt_string.replace('Eastern Standard Time', 'EST')\n",
    "    return dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742a6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean a single string\n",
    "def clean_string(s):\n",
    "    # Remove weird characters\n",
    "    pattern = r'[^a-zA-Z0-9 #,-]'\n",
    "    \n",
    "    return re.sub(pattern, '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28ff8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace multiple # with a single #\n",
    "def squeeze_hashes(s):\n",
    "    \n",
    "    # Matches one or more consecutive #\n",
    "    pattern = r'#+'  \n",
    "    return re.sub(pattern, '#', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74b6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just see if it starts with numbers like an address (123 Brighton street | Not Honolulu, HI 94666)\n",
    "def is_full_add(add_text):\n",
    "    ADD_REGEX = r'^\\d'\n",
    "    return re.search(ADD_REGEX, add_text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_address(input_str):\n",
    "    UNIT_REGEX = r'#\\S+'\n",
    "\n",
    "    # Parts of address\n",
    "    loc_name = '-'\n",
    "    street_add = '-'\n",
    "    rest = '-'\n",
    "    unit_num = '-'\n",
    "    city = '-'\n",
    "    state = '-'\n",
    "    zip_code = '-'\n",
    "\n",
    "    if input_str == 'Address unavailable':\n",
    "        return  loc_name, street_add, rest, unit_num, city, state, zip_code \n",
    "\n",
    "    add_parts = input_str.split('\\n')\n",
    "\n",
    "    for curr_part in add_parts:\n",
    "        # Check if the part contains any digits\n",
    "        if any(char.isdigit() for char in curr_part):\n",
    "            # If it contains digits, it's the address or rest of address with zip code\n",
    "\n",
    "            if is_full_add(curr_part):\n",
    "                street_add = curr_part\n",
    "            else:\n",
    "                rest = curr_part\n",
    "        else:\n",
    "            # If it doesn't contain digits, it's likely the location name\n",
    "            loc_name = curr_part\n",
    "\n",
    "    if street_add != '-':\n",
    "        if '#' in street_add:\n",
    "            \n",
    "            # We have an apartment number, so parse and set it\n",
    "            hash_idx = street_add.find('#')\n",
    "            unit_num = f'#{street_add[hash_idx:]}'\n",
    "\n",
    "    if rest != '-':\n",
    "        city, state_zip = rest.strip().split(',')\n",
    "\n",
    "        state, zip_code = state_zip.strip().split(' ')\n",
    "\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        print(input_str)\n",
    "        print(f\"\\tloc_name   =  {loc_name}\")\n",
    "        print(f\"\\tstreet_add =  {street_add}\")\n",
    "        print(f\"\\tunit_num   =  {unit_num}\")\n",
    "        print(f\"\\tcity       =  {city}\")\n",
    "        print(f\"\\tstate      =  {state}\")\n",
    "        print(f\"\\tzip_code   =  {zip_code}\")\n",
    "        print(\"============================================================\")\n",
    "\n",
    "\n",
    "\n",
    "    return  loc_name, street_add, rest, unit_num, city, state, zip_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23ce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(input_address, input_api_key):\n",
    "    try:\n",
    "        geolocator = GoogleV3(api_key = input_api_key)\n",
    "        location = geolocator.geocode(input_address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        else:\n",
    "            return None\n",
    "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc58890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.281983Z",
     "start_time": "2024-05-21T12:25:46.274631Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gen_url(page_num = '', option_id = '', min_dollar = '', max_dollar = ''):\n",
    "    \n",
    "    BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "    START_URL = f'{BASE_URL}/HBR/ForRent/?/Results/Neighborhood//'\n",
    "    MID_URL = f'/295//128////////'\n",
    "    SHORT_END_URL = '////////////////////////'\n",
    "\n",
    "    ret_url = f'{START_URL}{page_num}{MID_URL}1/{option_id}///{min_dollar}/{max_dollar}/{SHORT_END_URL}'\n",
    "    \n",
    "    return ret_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a5ecfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:25:46.273588Z",
     "start_time": "2024-05-21T12:25:46.262918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_page(input_url, input_region):\n",
    "\n",
    "    page = req.get(input_url)\n",
    "    soup = bs4(page.content, \"html.parser\")\n",
    "\n",
    "    curr_ids = [anchor.text for anchor in soup.find_all('a', string=re.compile(r'^\\d+$'))]\n",
    "\n",
    "    # Capture when there are additional pages of results\n",
    "    page_num_ls = list(set([x for x in curr_ids if x.isdigit() and 1 <= int(x) <= 10]))\n",
    "    n_pages = len(page_num_ls)\n",
    "\n",
    "    # Grab the IDs without the page numbers\n",
    "    curr_ids = list(set([x for x in curr_ids if x.isdigit() and 1 <= int(x) > 10]))\n",
    "    curr_id_ls = list(zip([region] * len(curr_ids), curr_ids))\n",
    "    curr_id_ls = [list(t) for t in curr_id_ls]\n",
    "    \n",
    "    \n",
    "    return [curr_id_ls, page_num_ls, n_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d1f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_page_w_api(input_id):\n",
    "\n",
    "    BASE_URL = 'https://propertysearch.hicentral.com'\n",
    "    curr_url = f'{BASE_URL}/HBR/ForRent/?/{input_id}'\n",
    "    \n",
    "    # Define the number of maximum retries\n",
    "    max_retries = 7\n",
    "    \n",
    "    # Create a Session object\n",
    "    session = req.Session()\n",
    "\n",
    "    # Setup retry strat\n",
    "    retry_strategy = Retry(total = 5, backoff_factor = 10)\n",
    "\n",
    "    # Create a HTTPAdapter with the specified max retries\n",
    "    adapter = HTTPAdapter(max_retries = retry_strategy)\n",
    "    \n",
    "    # Mount the adapter to the session\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    \n",
    "    page = session.get(curr_url)\n",
    "    soup = bs4(page.content, \"html.parser\")\n",
    "    \n",
    "    # Check if listing was not found\n",
    "    header = soup.find('h2', class_ = 'hdr-sub')\n",
    "    \n",
    "    \n",
    "    content_div = soup.find('div', id = 'content')\n",
    "    soup.contents = content_div.contents\n",
    "    \n",
    "    add_text = soup.h2.get_text(separator=\"\\n\")\n",
    "    \n",
    "    # Break address into parts \n",
    "    loc_name, street_add, rest, unit_num, city, state, zip_code  = break_address(add_text)\n",
    "\n",
    "    GOOD_ADD = False\n",
    "    if ((street_add != '-') and (city != '-') and (state != '-') and (zip_code != '-')):\n",
    "        clean_full_add = f'{street_add}, {city}, {state} {zip_code}'\n",
    "        GOOD_ADD = True\n",
    "    else:\n",
    "        clean_full_add = '-'\n",
    "    \n",
    "    \n",
    "    if GOOD_ADD:\n",
    "        lat, lon = get_coords(clean_full_add, GMAP_API_KEY)\n",
    "    else:\n",
    "        lat = '-'\n",
    "        lon = '-'\n",
    "    \n",
    "    \n",
    "    sub = soup.find('div', class_ = 'sub-heading')\n",
    "\n",
    "    active_status = sub.find('div', class_ = 'active-box')\n",
    "    active_status_str = active_status.text\n",
    "\n",
    "    price_boxes = sub.find_all('div', class_ = 'price-box')\n",
    "    for curr_box in price_boxes:\n",
    "\n",
    "\n",
    "        curr_label = curr_box.find('span').text\n",
    "\n",
    "\n",
    "        if curr_label == 'Price':\n",
    "            curr_price = curr_box.find('div').text\n",
    "\n",
    "        elif curr_label == 'Rental Type':\n",
    "            curr_type = curr_box.find('div').text\n",
    "\n",
    "        elif curr_label == 'Available Date':\n",
    "            curr_date = curr_box.find('div').text\n",
    "\n",
    "\n",
    "\n",
    "        bedrooms = soup.find('dt', string = 'Bedrooms: ').find_next_sibling('dd').text\n",
    "        full_baths = soup.find('dt', string = 'Full Baths: ').find_next_sibling('dd').text\n",
    "        half_baths = soup.find('dt', string = 'Half Baths: ').find_next_sibling('dd').text\n",
    "        parking = soup.find('dt', string = 'Parking Stalls: ').find_next_sibling('dd').text\n",
    "\n",
    "        land_area  = soup.find('dt', string = 'Land Area (sf): ').find_next_sibling('dd').text\n",
    "        live_area  = soup.find('dt', string = 'Living (sf): ').find_next_sibling('dd').text\n",
    "        lanai_area = soup.find('dt', string = 'Lanai (sf): ').find_next_sibling('dd').text\n",
    "        other_area = soup.find('dt', string = 'Other (sf): ').find_next_sibling('dd').text\n",
    "\n",
    "        island = soup.find('dt', string = 'Island:').find_next_sibling('dd').text.title()\n",
    "        region = soup.find('dt', string = 'Region:').find_next_sibling('dd').text.title()\n",
    "        hood = soup.find('dt', string = 'Neighborhood:').find_next_sibling('dd').text.title()\n",
    "\n",
    "        pets_allowed =  soup.find('dt', string = 'Pets Allowed?').find_next_sibling('dd').text\n",
    "        res_man =  soup.find('dt', string = 'Resident Manager?').find_next_sibling('dd').text\n",
    "\n",
    "\n",
    "        deposit =  soup.find('dt', string = 'Deposit Amount:').find_next_sibling('dd').text\n",
    "        term =  soup.find('dt', string = 'Terms Accepted:').find_next_sibling('dd').text\n",
    "\n",
    "        description = soup.find('h3', string = 'REMARKS:').find_next_sibling('p').text\n",
    "\n",
    "        unit_features =  soup.find('dt', string = 'Unit Features:').find_next_sibling('dd').text\n",
    "        parking_features =  soup.find('dt', string = 'Parking Features:').find_next_sibling('dd').text\n",
    "        frontage = soup.find('dt', string = 'Frontage:').find_next_sibling('dd').text\n",
    "        view = soup.find('dt', string = 'View:').find_next_sibling('dd').text\n",
    "        furnished = soup.find('dt', string = 'Furnished: ').find_next_sibling('dd').text\n",
    "        amenities = soup.find('dt', string = 'Amenities:').find_next_sibling('dd').text\n",
    "        pool = soup.find('dt', string = 'Pool:').find_next_sibling('dd').text\n",
    "        inclusions = soup.find('dt', string = 'Inclusions:').find_next_sibling('dd').text\n",
    "        build_style = soup.find('dt', string = 'Building Style:').find_next_sibling('dd').text\n",
    "        disclosures = soup.find('dt', string = 'Disclosures:').find_next_sibling('dd').text\n",
    "\n",
    "        img_urls = []\n",
    "        for img in soup.find_all('img', {'u': 'image'}):\n",
    "            img_urls.append(f\"https:{img['src']}\")\n",
    "\n",
    "    return [input_id, clean_full_add, GOOD_ADD, lat, lon, loc_name, \n",
    "                 street_add, unit_num, city, state, zip_code, active_status_str, curr_price, curr_type, curr_date,\n",
    "                 bedrooms, full_baths, half_baths, parking, \n",
    "                 land_area, live_area, lanai_area, other_area,\n",
    "                 island, region, hood, pets_allowed, res_man, deposit, term, description,\n",
    "                 unit_features, parking_features, frontage, view, furnished, amenities, pool, inclusions, \n",
    "                 build_style, disclosures, img_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348cd90",
   "metadata": {},
   "source": [
    "# Collect all property IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5f5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Staring collection of property IDs across 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:25<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get datetime\n",
    "curr_dt = get_dt()\n",
    "\n",
    "\n",
    "# It will max give me 10 pages of Apartments, so max of 200, so I'm going to need to \n",
    "# do some limitations \n",
    "\n",
    "# The regions defined on their site\n",
    "region_dict = {\n",
    "                    \"1\": \"All Regions\",\n",
    "                    \"2\": \"Diamond Head\",\n",
    "                    \"3\": \"Ewa Plain\",\n",
    "                    \"4\": \"Hawaii Kai\",\n",
    "                    \"5\": \"Kailua\",\n",
    "                    \"6\": \"Kaneohe\",\n",
    "                    \"7\": \"Leeward Coast\",\n",
    "                    \"8\": \"Makakilo\",\n",
    "                    \"9\": \"Metro Oahu\",\n",
    "                    \"10\": \"North Shore\",\n",
    "                    \"11\": \"Pearl City/Aiea\",\n",
    "                    \"12\": \"Waipahu\"\n",
    "}\n",
    "\n",
    "print(f'\\n\\nStaring collection of property IDs across {len(region_dict.keys())}')\n",
    "\n",
    "\n",
    "# If we get back over 200 results, we will play this game of slowly checking specific price ranges\n",
    "# so we can in the end get all results for that region\n",
    "DOLLAR_SEARCH_LS = [['', 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], \n",
    "                    [6, 7], [7, 8], [8, 9], [9, 10], [10, '']]\n",
    "\n",
    "# Keeps track of region name and property ID so we can look up all the details later\n",
    "fin_id_ls = []\n",
    "\n",
    "# A list to keep track of summary information, specifically for each region how many pages we processed\n",
    "# and how many property IDs we pulled from all those pages\n",
    "# ['Diamond Head', 2, 50],\n",
    "# ['Ewa Plain', 4, 89],\n",
    "# ['Hawaii Kai', 1, 22],\n",
    "# ['Kailua', 1, 35],\n",
    "# ['Kaneohe', 1, 40],\n",
    "# ...\n",
    "reg_sum_ls = []\n",
    "\n",
    "\n",
    "# Go through region by region\n",
    "for option_id, region in tqdm.tqdm(region_dict.items()):\n",
    "    \n",
    "  region_id_ls = []\n",
    "        \n",
    "  if DEBUG:\n",
    "    print(f'{option_id} -> {region}')\n",
    "        \n",
    "  # Skip the all regions option since we can't get all results\n",
    "  if region == 'All Regions':\n",
    "    continue\n",
    "        \n",
    "  # Generate our URL for this region\n",
    "  curr_url = gen_url(option_id = option_id)\n",
    "\n",
    "  # Get the ID list, page number list, and total number of pages for this region with no additional\n",
    "  # search criterion besides region.\n",
    "  curr_id_ls, page_num_ls, n_pages = get_page(curr_url, region)\n",
    "\n",
    "  # Keep count of the number of IDs we have for this region\n",
    "  n_ids = len(curr_id_ls)\n",
    "\n",
    "  # Also keep a list of region IDs we have been through\n",
    "  region_id_ls.append(curr_id_ls)\n",
    "\n",
    "  # Debug statements for printing info out when something goes wrong\n",
    "  if DEBUG:\n",
    "    print(f'\\tURL:\\t{curr_url}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "        \n",
    "        \n",
    "  # If we have more pages to look at grab the IDs for all those properties too\n",
    "  if len(page_num_ls) > 0:\n",
    "        \n",
    "    for curr_page in page_num_ls:\n",
    "\n",
    "      # Generate the url for this current page\n",
    "      curr_url = gen_url(page_num = curr_page, option_id = option_id)\n",
    "\n",
    "      # Get the list of IDs, etc.\n",
    "      curr_id_ls, page_num_ls, n_pages = get_page(curr_url, region)\n",
    "\n",
    "      # Update the number of IDs given this new page of results\n",
    "      n_ids = n_ids + len(curr_id_ls)\n",
    "      region_id_ls.append(curr_id_ls)\n",
    "\n",
    "      if DEBUG:\n",
    "        print(f'More than 1 page:\\n\\tURL:\\t{curr_url}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "        \n",
    "  # Metro oahu also has more than 200 properties, so we need to figure out a way\n",
    "  # to break that up \n",
    "  # We can walk different searches\n",
    "  if n_ids == 200:\n",
    "    # Example of what Page URL looks like including not just region but also min and max dollar\n",
    "    #PAGE_URL = f'{START_URL}{PAGE_NUM}{MID_URL}1/{option_id}///{MIN_DOLLAR}/MAX_DOLLAR/{SHORT_END_URL}'\n",
    "\n",
    "    # Go through that dollar search list we made before doing essentially what we have been doing, but adding\n",
    "    # prices to the gen_url arguments\n",
    "    for MIN_DOLLAR, MAX_DOLLAR in DOLLAR_SEARCH_LS:\n",
    "      # Reset page number\n",
    "      curr_url = gen_url(option_id = option_id, min_dollar = MIN_DOLLAR, max_dollar = MAX_DOLLAR)\n",
    "\n",
    "      curr_id_ls, page_num_ls, n_pages = get_page(curr_url, region)\n",
    "\n",
    "      n_ids = len(curr_id_ls)\n",
    "      region_id_ls.append(curr_id_ls)\n",
    "\n",
    "      if DEBUG:\n",
    "          print(f'More than 200 IDs:\\n\\tURL:\\t{curr_url}\\n\\tMin Dollar:\\t{MIN_DOLLAR}\\n\\tMax Dollar:\\t{MAX_DOLLAR}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "\n",
    "      # If we have more pages to look at grab those too\n",
    "      if len(page_num_ls) > 0:\n",
    "          \n",
    "        # Similar to above if we are going through price searches we need to be prepared that there might be more than 1 page \n",
    "        # for a set min and max price and we need to process that appropriately\n",
    "        for curr_page in page_num_ls:\n",
    "          curr_url = gen_url(page_num = curr_page, option_id = option_id,\n",
    "                              min_dollar = MIN_DOLLAR, max_dollar = MAX_DOLLAR)\n",
    "\n",
    "          curr_id_ls, page_num_ls, n_pages = get_page(curr_url, region)\n",
    "\n",
    "          n_ids = n_ids + len(curr_id_ls)\n",
    "          region_id_ls.append(curr_id_ls)\n",
    "\n",
    "        if DEBUG:\n",
    "          print(f'More than 200 IDs and more than 1 page:\\n\\tURL:\\t{curr_url}\\n\\tMin Dollar:\\t{MIN_DOLLAR}\\n\\tMax Dollar:\\t{MAX_DOLLAR}\\n\\tIDs:\\t{n_ids}\\n\\tPages:\\t{n_pages}')\n",
    "\n",
    "  reg_sum_ls.append([region, n_pages, n_ids])\n",
    "  fin_id_ls.append(region_id_ls)\n",
    "    \n",
    "if DEBUG:\n",
    "    print(\"==============================================================\")\n",
    "        \n",
    "# ChatGPT's interesting way of un-nesting this mess\n",
    "unnested_list = [inner for outer in fin_id_ls for middle in outer for inner in middle]\n",
    "fin_id_ls = unnested_list\n",
    "\n",
    "fin = pd.DataFrame(fin_id_ls, columns = ['region', 'id'])\n",
    "\n",
    "# Drop any dupes that were collected\n",
    "fin = fin.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29a2d7",
   "metadata": {},
   "source": [
    "# Get info about each property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e534719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T12:35:42.795761Z",
     "start_time": "2024-05-21T12:26:19.370828Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 682/682 [09:02<00:00,  1.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../web/assets/latest_home_data.tsv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the actual home data\n",
    "fin_id_ls = fin['id'].unique().tolist()\n",
    "\n",
    "home_data_ls = []\n",
    "failed_ls = []\n",
    "for curr_id in tqdm.tqdm(fin_id_ls):\n",
    "\n",
    "  curr_url = f'{BASE_URL}/HBR/ForRent/?/{curr_id}'\n",
    "  fail = True\n",
    "  try:\n",
    "    curr_page_dat = get_id_page_w_api(curr_id)\n",
    "    fail = False\n",
    "\n",
    "  except Exception as e:\n",
    "    if DEBUG:\n",
    "      print(f'Failed on {curr_id}:\\t{curr_url}\\n\\t\\t{e}')   \n",
    "\n",
    "    fail = True\n",
    "\n",
    "  if fail:\n",
    "    failed_ls.append(curr_id)\n",
    "  else:        \n",
    "    home_data_ls.append(curr_page_dat)\n",
    "        \n",
    "\n",
    "home_data = pd.DataFrame(home_data_ls, \n",
    "                        columns = ['input_id', 'clean_full_add', 'is_good_addy', 'lat', 'lon',\n",
    "                                   'loc_name', 'street_add', 'unit', 'city', 'state', 'zip_code', \n",
    "                                   'active_status_str', 'curr_price', 'curr_type', 'curr_date',\n",
    "                                   'bedrooms', 'full_baths', 'half_baths', 'parking', \n",
    "                                   'land_area', 'live_area', 'lanai_area', 'other_area',\n",
    "                                   'island', 'region', 'hood', 'pets_allowed', 'res_man', 'deposit', 'term', 'description',\n",
    "                                   'unit_features', 'parking_features', 'frontage', 'view', 'furnished', \n",
    "                                   'amenities', 'pool', 'inclusions', \n",
    "                                   'build_style', 'disclosures', 'img_urls'])\n",
    "\n",
    "\n",
    "# Little bit of clean up\n",
    "home_data['clean_full_add'] = home_data['clean_full_add'].apply(clean_string)\n",
    "home_data['street_add'] = home_data['street_add'].apply(clean_string)\n",
    "home_data['unit'] = home_data['unit'].apply(squeeze_hashes)\n",
    "\n",
    "\n",
    "# Remove any records without good address or lat/lon\n",
    "clean = home_data.copy(deep = True)\n",
    "clean = clean.loc[~((clean['is_good_addy'] == False) | \n",
    "                   (clean['lat'] == '-') | \n",
    "                   (clean['lat'] == '-')), ]\n",
    "\n",
    "\n",
    "RAW_FN   = f'{OUT_DIR}/hi_central_data_{curr_dt}.tsv'\n",
    "CLEAN_FN = f'{OUT_DIR}/hi_central_data_clean_{curr_dt}.tsv'\n",
    "FAIL_FN  = f'{OUT_DIR}/hi_central_failed_prop_id_list_{curr_dt}.tsv'\n",
    "\n",
    "# Save the output of this run\n",
    "home_data.to_csv(RAW_FN, sep = '\\t', index = False)\n",
    "clean.to_csv(CLEAN_FN, sep = '\\t', index = False)\n",
    "\n",
    "with open(FAIL_FN, 'w') as f:\n",
    "  for curr_fail in failed_ls:\n",
    "    f.write(f\"{curr_fail}\\n\")\n",
    "\n",
    "# Create a copy of clean data in web/assets/ for use with website\n",
    "shutil.copy2(CLEAN_FN, f'../../web/assets/latest_home_data.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb303b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../web/assets/latest_home_data.tsv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_FN   = f'{OUT_DIR}/hi_central_data_{curr_dt}.tsv'\n",
    "CLEAN_FN = f'{OUT_DIR}/hi_central_data_clean_{curr_dt}.tsv'\n",
    "FAIL_FN  = f'{OUT_DIR}/hi_central_failed_prop_id_list_{curr_dt}.tsv'\n",
    "\n",
    "# Save the output of this run\n",
    "home_data.to_csv(RAW_FN, sep = '\\t', index = False)\n",
    "clean.to_csv(CLEAN_FN, sep = '\\t', index = False)\n",
    "\n",
    "with open(FAIL_FN, 'w') as f:\n",
    "  for curr_fail in failed_ls:\n",
    "    f.write(f\"{curr_fail}\\n\")\n",
    "\n",
    "# Create a copy of clean data in web/assets/ for use with website\n",
    "shutil.copy2(CLEAN_FN, f'../../web/assets/latest_home_data.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
